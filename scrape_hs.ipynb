{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tables (Skills)__  \n",
    "  \n",
    "0 - Overall  \n",
    "1 - Attack  \n",
    "2 - Defence  \n",
    "3 - Strength  \n",
    "4 - Hitpoints  \n",
    "5 - Ranged  \n",
    "6 - Prayer  \n",
    "7 - Magic  \n",
    "8 - Cooking  \n",
    "9 - Woodcutting  \n",
    "10 - Fletching  \n",
    "11 - Fishing  \n",
    "12 - Firemaking  \n",
    "13 - Crafting  \n",
    "14 - Smithing  \n",
    "15 - Mining  \n",
    "16 - Herblore  \n",
    "17 - Agility  \n",
    "18 - Thieving  \n",
    "19 - Slayer  \n",
    "20 - Farming  \n",
    "21 - Runecraft  \n",
    "22 - Hunter  \n",
    "23 - Construction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas\n",
    "import sqlite3 as sql\n",
    "from bs4 import BeautifulSoup\n",
    "import pyspark\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"osrs\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#conf = pyspark.SparkConf()\n",
    "#sc = pyspark.SparkContext(conf.setAppName(\"osrs\"))\n",
    "\n",
    "#sc = pyspark.SparkContext(\"osrs\")\n",
    "#sqlContext = pyspark.sql.SQLContext(spark)\n",
    "#sc = pyspark.SparkContext\n",
    "#sc.broadcast(url, 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load('data/player_names.parquet').repartition(8)\n",
    "#rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='IDK Hunter'),\n",
       " Row(name='Evil rake'),\n",
       " Row(name='Hoibur'),\n",
       " Row(name='madnesh'),\n",
       " Row(name='AGM-86')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = df.select('name').limit(5)\n",
    "players.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_player(players):\n",
    "    from datetime import datetime\n",
    "    #print(player['name'])\n",
    "    # base url for api request\n",
    "    player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='\n",
    "    \n",
    "    for player in players:\n",
    "        name = player['name']\n",
    "        url = player_url_base + name\n",
    "    \n",
    "        now = datetime.utcnow()\n",
    "        time = '{0}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        \n",
    "        try:\n",
    "            page = requests.get(url).text.replace(u'\\n', u' ')\n",
    "            skills = [i.split(',') for i in page.split()]\n",
    "            xp = [i[2] for i in skills[0:24]]\n",
    "            rank = [i[0] for i in skills[0:24]]\n",
    "        except:\n",
    "            print(name)\n",
    "            continue\n",
    "       \n",
    "    return (name, time, xp, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_player_part(players):\n",
    "    from datetime import datetime\n",
    "    #print(player['name'])\n",
    "    # base url for api request\n",
    "    player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='\n",
    "    \n",
    "    players_data = []\n",
    "    \n",
    "    for player in players:\n",
    "        name = player['name']\n",
    "        url = player_url_base + name\n",
    "    \n",
    "        now = datetime.utcnow()\n",
    "        time = '{0}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        \n",
    "        try:\n",
    "            page = requests.get(url).text.replace(u'\\n', u' ')\n",
    "            skills = [i.split(',') for i in page.split()]\n",
    "            xp = [i[2] for i in skills[0:24]]\n",
    "            rank = [i[0] for i in skills[0:24]]\n",
    "            players_data.append(tuple((name, time, xp, rank)))\n",
    "        except:\n",
    "            print(name)\n",
    "            continue\n",
    "            \n",
    "    yield players_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint enabled:  True \n",
      "Num partitions:  8\n"
     ]
    }
   ],
   "source": [
    "players = df.select('name').repartition(8)\n",
    "players_hs = players.rdd.mapPartitions(get_single_player_part)\n",
    "players_hs.is_checkpointed = True\n",
    "print('Checkpoint enabled: ', players_hs.is_checkpointed,'\\nNum partitions: ', players_hs.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 23:03:50.139876\n",
      "2019-05-23 23:05:02.642486\n"
     ]
    }
   ],
   "source": [
    "print(datetime.utcnow())\n",
    "players_results = players_hs.collect()\n",
    "print(datetime.utcnow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(players_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tyler', 100, 'att')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Tyler'\n",
    "xp = 100\n",
    "skill = 'att'\n",
    "tuple((name, xp, skill))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for i in range(len(players_results)):\n",
    "    total += len(players_results[i])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7c61db97262e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rdd' is not defined"
     ]
    }
   ],
   "source": [
    "rdd.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = players.rdd\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'defaultParallelism'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-53bc4e5e8d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \"\"\"\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mnumSlices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'defaultParallelism'"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.parallelize(rdd, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RDD.checkpoint of MapPartitionsRDD[45] at javaToPython at NativeMethodAccessorImpl.java:0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "accumulator() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-328b02f072e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: accumulator() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "num = sc.accumulator(1)\n",
    "def f(x):\n",
    "    global num\n",
    "    num+=x\n",
    "rdd = sc.parallelize([20, 30, 40, 50])\n",
    "rdd.foreach(f)\n",
    "final = num.value\n",
    "print('acc value is: {0}'.format(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='IDK Hunter'),\n",
       " Row(name='Evil rake'),\n",
       " Row(name='Hoibur'),\n",
       " Row(name='madnesh'),\n",
       " Row(name='AGM-86'),\n",
       " Row(name='NoBotsPlease'),\n",
       " Row(name='Intu'),\n",
       " Row(name='Mr Carl'),\n",
       " Row(name='Dark Zatana'),\n",
       " Row(name='Jerry PvMs')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [players.collect()][0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player(conn, skill):\n",
    "    # base url for api request\n",
    "    player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='\n",
    "    \n",
    "    skills = [\"Overall\", \"Attack\", \"Defence\", \"Strength\", \"Hitpoints\", \"Ranged\", \"Prayer\", \"Magic\", \"Cooking\",\n",
    "              \"Woodcutting\", \"Fletching\", \"Fishing\", \"Firemaking\", \"Crafting\", \"Smithing\", \"Mining\", \"Herblore\",\n",
    "              \"Agility\", \"Thieving\", \"Slayer\", \"Farming\", \"Runecraft\", \"Hunter\", \"Construction\"]\n",
    "    skill_name = skills[skill]\n",
    "    print(skill_name)\n",
    "    \n",
    "    players = conn.execute('''SELECT name from players_name WHERE skill = (?)''', (skill_name,)).fetchall()\n",
    "    num_players = len(players)\n",
    "    \n",
    "    now = datetime.utcnow()\n",
    "    time = '{0}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(now.year, now.month, now.day, \n",
    "                                                                now.hour, now.minute, now.second)\n",
    "    failed = []\n",
    "    num = 0\n",
    "    \n",
    "    for player in players:\n",
    "        # sql returns list of tuples, this isolates the name\n",
    "        name = player[0]\n",
    "        url = player_url_base + name\n",
    "        \n",
    "        num += 1\n",
    "        if not num%20:\n",
    "            print('Players scraped: {0}/{1}'.format(i, num_players), datetime.utcnow() - now)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            page = requests.get(url).text.replace(u'\\n', u' ')\n",
    "            skills = [i.split(',') for i in page.split()]\n",
    "            xp = [i[2] for i in skills[0:24]]\n",
    "            rank = [i[0] for i in skills[0:24]]\n",
    "        except:\n",
    "            print(name)\n",
    "            failed.append(name)\n",
    "            continue\n",
    "            \n",
    "        conn.execute('''INSERT INTO players_xp VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''', [name, time] + xp)\n",
    "        conn.execute('''INSERT INTO players_rank VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''', [name, time] + rank)\n",
    "            \n",
    "    return failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(date,DateType,true),StructField(skill,StringType,true),StructField(rank,LongType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "df = df.withColumn('date', df['date'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o166.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 11, localhost, executor driver): java.io.FileNotFoundException: File file:/Users/tylerblair/Documents/projects/bot_detection/data/player_names.parquet/part-00016-35538ec8-3aa5-4c02-9994-b09ed5ae9ea5-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/tylerblair/Documents/projects/bot_detection/data/player_names.parquet/part-00016-35538ec8-3aa5-4c02-9994-b09ed5ae9ea5-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4cbe78e18809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/player_names.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o166.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 11, localhost, executor driver): java.io.FileNotFoundException: File file:/Users/tylerblair/Documents/projects/bot_detection/data/player_names.parquet/part-00016-35538ec8-3aa5-4c02-9994-b09ed5ae9ea5-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/tylerblair/Documents/projects/bot_detection/data/player_names.parquet/part-00016-35538ec8-3aa5-4c02-9994-b09ed5ae9ea5-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.write.mode('overwrite').parquet('data/player_names.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input '/' expecting <EOF>(line 1, pos 8)\\n\\n== SQL ==\\nuse data/osrs.db\\n--------^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input '/' expecting <EOF>(line 1, pos 8)\n\n== SQL ==\nuse data/osrs.db\n--------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7ad96c74dc6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'use data/osrs.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input '/' expecting <EOF>(line 1, pos 8)\\n\\n== SQL ==\\nuse data/osrs.db\\n--------^^^\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql('use data/osrs.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-914b8e68f496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use data/osrs.db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import pyspark as spark\n",
    "spark.sql(\"use data/osrs.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Overall',\n",
       " 'Attack',\n",
       " 'Defence',\n",
       " 'Strength',\n",
       " 'Hitpoints',\n",
       " 'Ranged',\n",
       " 'PrayerMagic',\n",
       " 'Cooking',\n",
       " 'Woodcutting',\n",
       " 'Fletching',\n",
       " 'Fishing',\n",
       " 'Firemaking',\n",
       " 'Crafting',\n",
       " 'Smithing',\n",
       " 'Mining',\n",
       " 'Herblore',\n",
       " 'Agility',\n",
       " 'Thieving',\n",
       " 'Slayer',\n",
       " 'Farming',\n",
       " 'Runecraft',\n",
       " 'Hunter',\n",
       " 'Construction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subtract 3 from line number to get table number, 0-23\n",
    "#e.g., table 9 is woodcutting\n",
    "skills = [\"Overall\",\n",
    "          \"Attack\",\n",
    "          \"Defence\",\n",
    "          \"Strength\",\n",
    "          \"Hitpoints\",\n",
    "          \"Ranged\",\n",
    "          \"Prayer\"\n",
    "          \"Magic\",\n",
    "          \"Cooking\",\n",
    "          \"Woodcutting\",\n",
    "          \"Fletching\",\n",
    "          \"Fishing\",\n",
    "          \"Firemaking\",\n",
    "          \"Crafting\",\n",
    "          \"Smithing\",\n",
    "          \"Mining\",\n",
    "          \"Herblore\",\n",
    "          \"Agility\",\n",
    "          \"Thieving\",\n",
    "          \"Slayer\",\n",
    "          \"Farming\",\n",
    "          \"Runecraft\",\n",
    "          \"Hunter\",\n",
    "          \"Construction\"]\n",
    "skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get player names and insert into osrs.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_players_table(conn):\n",
    "    \n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS players(\n",
    "                    \"name\" TEXT PRIMARY KEY,\n",
    "                    \"date\" DATETIME NOT NULL,\n",
    "                    \"skill\" TEXT,\n",
    "                    \"rank\" INTEGER)''')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_names(min_rank, max_rank, skill):\n",
    "    import requests\n",
    "    import sqlite3 as sql\n",
    "    from datetime import datetime\n",
    "    from bs4 import BeautifulSoup\n",
    "    import hs_tables\n",
    "    \n",
    "    conn = sql.connect('data/osrs.db')\n",
    "    \n",
    "    # create players table if it doesn't already exist\n",
    "    # attributes: name, date, skill\n",
    "    hs_tables.create_players_table(conn)\n",
    "    \n",
    "    # get skill name for inserting attribute in datebase table\n",
    "    skills = [\"Overall\", \"Attack\", \"Defence\", \"Strength\", \"Hitpoints\", \"Ranged\", \"Prayer\", \"Magic\", \"Cooking\",\n",
    "              \"Woodcutting\", \"Fletching\", \"Fishing\", \"Firemaking\", \"Crafting\", \"Smithing\", \"Mining\", \"Herblore\",\n",
    "              \"Agility\", \"Thieving\", \"Slayer\", \"Farming\", \"Runecraft\", \"Hunter\", \"Construction\"]\n",
    "    skill_name = skills[skill]\n",
    "    \n",
    "    # time in utc for later time series analysis\n",
    "    now = datetime.utcnow()\n",
    "    time = '{0}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(now.year, now.month, now.day, \n",
    "                                                                now.hour, now.minute, now.second)\n",
    "    \n",
    "    total_base = 'https://secure.runescape.com/m=hiscore_oldschool/overall.ws?table={0}&page='.format(skill)\n",
    "    #player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='\n",
    "    \n",
    "    # range of pages to scrape results\n",
    "    min_page = min_rank//25\n",
    "    max_page = max_rank//25\n",
    "    pages = max_page - min_page\n",
    "    #url_no_page = ranking_url_base + 'table={22}&page='.format(table_num)\n",
    "    \n",
    "    for i in range(min_page, max_page):\n",
    "        \n",
    "        url = total_base + str(i)\n",
    "        soup = BeautifulSoup(requests.get(url).text)\n",
    "        try:\n",
    "            table = soup.find(\"tbody\")\n",
    "        # create catch here\n",
    "        except:\n",
    "            return soup\n",
    "        \n",
    "        for row in table.findAll(\"tr\"):\n",
    "            player = row.findAll(\"td\")\n",
    "            # cols = [rank, name, level, xp]\n",
    "            cols = [element.text.strip() for element in player]\n",
    "            #name = cols[1].replace(u'\\xa0', u' ')\n",
    "            \n",
    "            if cols[1]:\n",
    "                name = cols[1].replace(u'\\xa0', u' ')\n",
    "                #print(name)\n",
    "                rank = int(cols[0].replace(u',', u''))\n",
    "                try:\n",
    "                    conn.execute('''INSERT OR IGNORE INTO players(name, date, skill, rank) VALUES (?,?,?,?)''',(name, time, skill_name, int(rank)))\n",
    "                    conn.commit()\n",
    "                except:\n",
    "                    print('Failed:', cols)\n",
    "                    continue\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        if not i%20:\n",
    "            print('Pages scraped: {0}/{1}'.format(i-min_page, pages), datetime.utcnow() - now)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages scraped: 0/3040 2019-05-23 00:42:32.248702\n",
      "Pages scraped: 10/3040 2019-05-23 00:42:52.417858\n",
      "Pages scraped: 90/3040 2019-05-23 00:47:09.248114\n",
      "Pages scraped: 110/3040 2019-05-23 00:48:39.260316\n",
      "Pages scraped: 130/3040 2019-05-23 00:50:06.607790\n",
      "Pages scraped: 150/3040 2019-05-23 00:51:33.753334\n",
      "Pages scraped: 160/3040 2019-05-23 00:52:19.528423\n",
      "Pages scraped: 180/3040 2019-05-23 00:53:51.589447\n",
      "Pages scraped: 250/3040 2019-05-23 00:58:54.393519\n",
      "Pages scraped: 280/3040 2019-05-23 01:01:07.680285\n",
      "Pages scraped: 300/3040 2019-05-23 01:02:33.434284\n",
      "Pages scraped: 310/3040 2019-05-23 01:03:18.902874\n",
      "Pages scraped: 430/3040 2019-05-23 01:12:08.331737\n",
      "Pages scraped: 440/3040 2019-05-23 01:12:48.781933\n",
      "Pages scraped: 450/3040 2019-05-23 01:13:35.068626\n",
      "Pages scraped: 470/3040 2019-05-23 01:15:06.100727\n",
      "Pages scraped: 480/3040 2019-05-23 01:15:51.974203\n",
      "Pages scraped: 500/3040 2019-05-23 01:17:18.609096\n",
      "Pages scraped: 600/3040 2019-05-23 01:24:39.643897\n",
      "Pages scraped: 620/3040 2019-05-23 01:26:18.975955\n",
      "Pages scraped: 680/3040 2019-05-23 01:30:38.570244\n",
      "Pages scraped: 690/3040 2019-05-23 01:31:19.121981\n",
      "Pages scraped: 700/3040 2019-05-23 01:32:04.282481\n",
      "Pages scraped: 710/3040 2019-05-23 01:32:49.854168\n",
      "Pages scraped: 730/3040 2019-05-23 01:34:18.425911\n",
      "Pages scraped: 760/3040 2019-05-23 01:36:35.648994\n",
      "Pages scraped: 770/3040 2019-05-23 01:37:21.219852\n",
      "Pages scraped: 850/3040 2019-05-23 01:43:09.267112\n",
      "Pages scraped: 870/3040 2019-05-23 01:44:36.717205\n",
      "Pages scraped: 880/3040 2019-05-23 01:45:22.592324\n",
      "Pages scraped: 890/3040 2019-05-23 01:46:03.450991\n",
      "Pages scraped: 900/3040 2019-05-23 01:46:48.915323\n",
      "Pages scraped: 980/3040 2019-05-23 01:52:39.646262\n",
      "Pages scraped: 1000/3040 2019-05-23 01:54:05.052348\n",
      "Pages scraped: 1010/3040 2019-05-23 01:54:49.903425\n",
      "Pages scraped: 1090/3040 2019-05-23 02:00:39.101755\n",
      "Pages scraped: 1110/3040 2019-05-23 02:02:05.323085\n",
      "Pages scraped: 1120/3040 2019-05-23 02:02:50.788948\n",
      "Pages scraped: 1200/3040 2019-05-23 02:08:39.779021\n",
      "Pages scraped: 1220/3040 2019-05-23 02:10:06.312838\n",
      "Pages scraped: 1240/3040 2019-05-23 02:11:33.762198\n",
      "Pages scraped: 1250/3040 2019-05-23 02:12:19.746465\n",
      "Pages scraped: 1350/3040 2019-05-23 02:19:39.358279\n",
      "Pages scraped: 1370/3040 2019-05-23 02:21:07.339486\n",
      "Pages scraped: 1380/3040 2019-05-23 02:21:48.491625\n",
      "Pages scraped: 1390/3040 2019-05-23 02:22:34.982584\n",
      "Pages scraped: 1400/3040 2019-05-23 02:23:20.961867\n",
      "Pages scraped: 1430/3040 2019-05-23 02:25:39.207864\n",
      "Pages scraped: 1450/3040 2019-05-23 02:27:07.786251\n",
      "Pages scraped: 1480/3040 2019-05-23 02:29:18.860441\n",
      "Pages scraped: 1590/3040 2019-05-23 02:37:24.560275\n",
      "Pages scraped: 1620/3040 2019-05-23 02:39:36.865754\n",
      "Pages scraped: 1640/3040 2019-05-23 02:41:04.523556\n",
      "Pages scraped: 1650/3040 2019-05-23 02:41:50.502901\n",
      "Pages scraped: 1670/3040 2019-05-23 02:43:24.407169\n",
      "Pages scraped: 1700/3040 2019-05-23 02:45:36.507640\n",
      "Pages scraped: 1710/3040 2019-05-23 02:46:22.277186\n",
      "Pages scraped: 1810/3040 2019-05-23 02:53:37.803639\n",
      "Pages scraped: 1820/3040 2019-05-23 02:54:18.555737\n",
      "Pages scraped: 1830/3040 2019-05-23 02:55:03.923470\n",
      "Pages scraped: 1840/3040 2019-05-23 02:55:49.288732\n",
      "Pages scraped: 1920/3040 2019-05-23 03:01:38.688541\n",
      "Pages scraped: 1930/3040 2019-05-23 03:02:19.447409\n",
      "Pages scraped: 1940/3040 2019-05-23 03:03:05.633496\n",
      "Pages scraped: 1950/3040 2019-05-23 03:03:51.300234\n",
      "Pages scraped: 2040/3040 2019-05-23 03:10:24.633176\n",
      "Pages scraped: 2070/3040 2019-05-23 03:12:36.119876\n",
      "Pages scraped: 2080/3040 2019-05-23 03:13:21.586848\n",
      "Pages scraped: 2180/3040 2019-05-23 03:20:37.515503\n",
      "Pages scraped: 2200/3040 2019-05-23 03:22:03.842340\n",
      "Pages scraped: 2220/3040 2019-05-23 03:23:36.720752\n",
      "Pages scraped: 2230/3040 2019-05-23 03:24:22.085390\n",
      "Pages scraped: 2310/3040 2019-05-23 03:30:08.824667\n",
      "Pages scraped: 2330/3040 2019-05-23 03:31:36.387068\n",
      "Pages scraped: 2340/3040 2019-05-23 03:32:22.460849\n",
      "Pages scraped: 2350/3040 2019-05-23 03:33:03.526869\n",
      "Pages scraped: 2360/3040 2019-05-23 03:33:49.499303\n",
      "Pages scraped: 2460/3040 2019-05-23 03:41:06.457701\n",
      "Pages scraped: 2470/3040 2019-05-23 03:41:51.719846\n",
      "Pages scraped: 2560/3040 2019-05-23 03:48:24.538605\n",
      "Pages scraped: 2570/3040 2019-05-23 03:49:09.596735\n",
      "Pages scraped: 2590/3040 2019-05-23 03:50:38.050681\n",
      "Pages scraped: 2610/3040 2019-05-23 03:52:03.672098\n",
      "Pages scraped: 2620/3040 2019-05-23 03:52:49.226724\n",
      "Pages scraped: 2700/3040 2019-05-23 03:58:37.186350\n",
      "Pages scraped: 2710/3040 2019-05-23 03:59:22.138493\n",
      "Pages scraped: 2880/3040 2019-05-23 04:11:54.639228\n",
      "Pages scraped: 2890/3040 2019-05-23 04:12:39.185666\n",
      "Pages scraped: 2910/3040 2019-05-23 04:14:04.694256\n",
      "Pages scraped: 2920/3040 2019-05-23 04:14:49.956095\n",
      "Pages scraped: 3000/3040 2019-05-23 04:20:38.843720\n",
      "Pages scraped: 3010/3040 2019-05-23 04:21:19.500008\n",
      "Pages scraped: 3020/3040 2019-05-23 04:22:05.271707\n",
      "Pages scraped: 3030/3040 2019-05-23 04:22:50.944870\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import importlib\n",
    "    importlib.reload(scrape_hs)\n",
    "    print('Reimported scrape_hs.py')\n",
    "except:\n",
    "    import scrape_hs\n",
    "\n",
    "# 1400 end, 35000 done\n",
    "min_rank = 485000 #440000 min originally\n",
    "max_rank = 561000\n",
    "skill = 23 # construction = 23\n",
    "\n",
    "table = scrape_hs.scrape_names(min_rank, max_rank, skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47365"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = conn.execute('''SELECT name from players_name''').fetchall()\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "conn.execute('''CREATE TABLE IF NOT EXISTS players_name(\n",
    "                    \"name\" TEXT, \n",
    "                    \"date\" DATETIME NOT NULL, \n",
    "                    \"skill\" TEXT, \n",
    "                    \"rank\" INTEGER)''')\n",
    "print('1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'name', 'TEXT', 0, None, 0),\n",
       " (1, 'date', 'DATETIME', 1, None, 0),\n",
       " (2, 'skill', 'TEXT', 0, None, 0),\n",
       " (3, 'rank', 'INTEGER', 0, None, 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute('''PRAGMA table_info(\"players_name\")''').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!doctype html>\\n<!--[if lt IE 7]><html class=\"no-js lt-ie10 lt-ie9 lt-ie8 lt-ie7\" lang=\"en\"><![endi'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://secure.runescape.com/m=hiscore_oldschool/overall.ws?table=23&page=2000'\n",
    "requests.get(url).text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>skill</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, date, skill, rank]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'name': [],\n",
    "             'date': [],\n",
    "             'skill': [],\n",
    "             'rank': []})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>skill</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tyler</td>\n",
       "      <td>now</td>\n",
       "      <td>abc</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tyler</td>\n",
       "      <td>now</td>\n",
       "      <td>abc</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name date skill   rank\n",
       "0  Tyler  now   abc  100.0\n",
       "1  Tyler  now   abc  100.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.append({'name': 'Tyler', 'date': 'now', 'skill': 'abc', 'rank': 100}, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hellz Zerker', '2019-05-22 19:34:35', 'Construction', 439976),\n",
       " ('Grant2k1', '2019-05-22 19:34:35', 'Construction', 439977),\n",
       " ('Kyle SR', '2019-05-22 19:34:35', 'Construction', 439978),\n",
       " ('Supreme_v1', '2019-05-22 19:34:35', 'Construction', 439979),\n",
       " ('M3EK MlLLY', '2019-05-22 19:34:35', 'Construction', 439980),\n",
       " ('Von Disney', '2019-05-22 19:34:35', 'Construction', 439981),\n",
       " ('ChrisW247', '2019-05-22 19:34:35', 'Construction', 439982),\n",
       " ('CleanedTotal', '2019-05-22 19:34:35', 'Construction', 439983),\n",
       " ('buga shuga', '2019-05-22 19:34:35', 'Construction', 439984),\n",
       " ('OBG Nicola', '2019-05-22 19:34:35', 'Construction', 439985),\n",
       " ('M d m a zing', '2019-05-22 19:34:35', 'Construction', 439986),\n",
       " ('MrTrunks', '2019-05-22 19:34:35', 'Construction', 439987),\n",
       " ('Logless', '2019-05-22 19:34:35', 'Construction', 439988),\n",
       " ('thafamilia3', '2019-05-22 19:34:35', 'Construction', 439989),\n",
       " ('SSGvegito', '2019-05-22 19:34:35', 'Construction', 439990),\n",
       " ('Space Mankey', '2019-05-22 19:34:35', 'Construction', 439991),\n",
       " ('PhantomsCore', '2019-05-22 19:34:35', 'Construction', 439992),\n",
       " ('young pimpin', '2019-05-22 19:34:35', 'Construction', 439993),\n",
       " ('lceef', '2019-05-22 19:34:35', 'Construction', 439994),\n",
       " ('sweetslaps', '2019-05-22 19:34:35', 'Construction', 439995),\n",
       " ('redromper', '2019-05-22 19:34:35', 'Construction', 439996),\n",
       " ('TMGGuthan', '2019-05-22 19:34:35', 'Construction', 439997),\n",
       " ('Stab Em All2', '2019-05-22 19:34:35', 'Construction', 439998),\n",
       " ('Badger64', '2019-05-22 19:34:35', 'Construction', 439999),\n",
       " ('YES vote', '2019-05-22 19:34:35', 'Construction', 440000)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute('''SELECT * from players''').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x114fcd730>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.close()\n",
    "conn = sql.connect('data/osrs.db')\n",
    "\n",
    "conn.execute('''DROP TABLE players''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1558553079.964841"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MattsMauler',\n",
       "  'Zen Santa',\n",
       "  'Dr McGroober',\n",
       "  'Bird Brained',\n",
       "  'BoxOfCondums',\n",
       "  'Brother Boof',\n",
       "  'Typulsion',\n",
       "  'Boyo Bob',\n",
       "  'High Pro',\n",
       "  'Emperator',\n",
       "  'Mabaza',\n",
       "  'Tokkurainen',\n",
       "  'Akaaay',\n",
       "  'leathalchick',\n",
       "  'KinG CarrotI',\n",
       "  'Truitedulac3',\n",
       "  'Lightworks',\n",
       "  'Lukeed113',\n",
       "  '100 hp',\n",
       "  'Sushi Phil',\n",
       "  'HappinessLTD',\n",
       "  'RevToo',\n",
       "  'zdog520',\n",
       "  'Embezzlez',\n",
       "  'ICESKATE'],\n",
       " ['440,000', 'ICESKATE', '51', '112,073'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_hs_tables(conn):\n",
    "    \n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS player_xp (\n",
    "                    name TEXT PRIMARY KEY,\n",
    "                    date DATETIME NOT NULL,\n",
    "                    overall INTEGER,\n",
    "                    attack INTEGER,\n",
    "                    defence INTEGER,\n",
    "                    strength INTEGER,\n",
    "                    hitpoints INTEGER,\n",
    "                    ranged INTEGER,\n",
    "                    prayer INTEGER,\n",
    "                    magic INTEGER,\n",
    "                    cooking INTEGER,\n",
    "                    woodcutting INTEGER,\n",
    "                    fletching INTEGER,\n",
    "                    fishing INTEGER,\n",
    "                    firemaking INTEGER,\n",
    "                    crafting INTEGER,\n",
    "                    smithing INTEGER,\n",
    "                    mining INTEGER,\n",
    "                    herblore INTEGER,\n",
    "                    agility INTEGER,\n",
    "                    thieving INTEGER,\n",
    "                    slayer INTEGER,\n",
    "                    farming INTEGER,\n",
    "                    runecraft INTEGER,\n",
    "                    hunter INTEGER,\n",
    "                    construction INTEGER)''')\n",
    "\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS player_rank (\n",
    "                    name TEXT PRIMARY KEY,\n",
    "                    date DATETIME NOT NULL,\n",
    "                    overall INTEGER,\n",
    "                    attack INTEGER,\n",
    "                    defence INTEGER,\n",
    "                    strength INTEGER,\n",
    "                    hitpoints INTEGER,\n",
    "                    ranged INTEGER,\n",
    "                    prayer INTEGER,\n",
    "                    magic INTEGER,\n",
    "                    cooking INTEGER,\n",
    "                    woodcutting INTEGER,\n",
    "                    fletching INTEGER,\n",
    "                    fishing INTEGER,\n",
    "                    firemaking INTEGER,\n",
    "                    crafting INTEGER,\n",
    "                    smithing INTEGER,\n",
    "                    mining INTEGER,\n",
    "                    herblore INTEGER,\n",
    "                    agility INTEGER,\n",
    "                    thieving INTEGER,\n",
    "                    slayer INTEGER,\n",
    "                    farming INTEGER,\n",
    "                    runecraft INTEGER,\n",
    "                    hunter INTEGER,\n",
    "                    construction INTEGER)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 5, 22, 18, 5, 33, 113844)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.utcnow()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-05-22 18:05:33'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = '{0}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 5, 22, 18, 3, 43, 340375)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = datetime.datetime.utcnow()\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_num from skills list above, e.g., table 9 = woodcutting\n",
    "def scrape_rank(table_num, num_players):\n",
    "    \n",
    "    conn = sql.connect('osrs_hs.db')\n",
    "    \n",
    "    max_pages = num_players//25\n",
    "    url_no_page = ranking_url_base + 'table={0}&page='.format(table_num)\n",
    "    \n",
    "    for i in range(max_pages):\n",
    "        url = url_no_page + str(i)\n",
    "        soup = BeautifulSoup(requests.get(url).text)\n",
    "        try:\n",
    "            table = soup.find(\"tbody\")\n",
    "        except:\n",
    "            return players\n",
    "        \n",
    "        for row in table.findAll(\"tr\"):\n",
    "            player = row.findAll(\"td\")\n",
    "            cols = [element.text.strip() for element in player]\n",
    "            players.append(cols[1])\n",
    "            \n",
    "    return players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base URLs for later queries\n",
    "# ranking URL ends in: table={0}&page={1} where table number is in order of skills list\n",
    "ranking_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/overall.ws?' \n",
    "player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
