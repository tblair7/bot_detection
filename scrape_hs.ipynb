{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='top'> OSRS Bot Detection</a>\n",
    "----  \n",
    "Author: [Tyler Blair](https://github.com/tblair7)\n",
    "\n",
    "__Go to:__  \n",
    "<a href=#scrape_names>Scrape names</a>  \n",
    "<a href=#scrape_skills>Scrape skills of all names</a>  \n",
    "<a href=#skillstable>Skills table</a>  \n",
    "<a href=#old>Deprecated work</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to top</a>  \n",
    "### <a name='scrape_names'> Scrape names from hiscores </a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_names(min_rank, max_rank, skill):\n",
    "    import requests\n",
    "    import sqlite3 as sql\n",
    "    from datetime import datetime\n",
    "    from bs4 import BeautifulSoup\n",
    "    import hs_tables\n",
    "    \n",
    "    conn = sql.connect('data/osrs.db')\n",
    "    \n",
    "    # create players table if it doesn't already exist\n",
    "    # attributes: name, date, skill\n",
    "    hs_tables.create_players_table(conn)\n",
    "    \n",
    "    # get skill name for inserting attribute in datebase table\n",
    "    skills = [\"Overall\", \"Attack\", \"Defence\", \"Strength\", \"Hitpoints\", \"Ranged\", \"Prayer\", \"Magic\", \"Cooking\",\n",
    "              \"Woodcutting\", \"Fletching\", \"Fishing\", \"Firemaking\", \"Crafting\", \"Smithing\", \"Mining\", \"Herblore\",\n",
    "              \"Agility\", \"Thieving\", \"Slayer\", \"Farming\", \"Runecraft\", \"Hunter\", \"Construction\"]\n",
    "    skill_name = skills[skill]\n",
    "    \n",
    "    # time in utc for later time series analysis\n",
    "    now = datetime.utcnow()\n",
    "    time = '{0}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(now.year, now.month, now.day, \n",
    "                                                                now.hour, now.minute, now.second)\n",
    "    \n",
    "    total_base = 'https://secure.runescape.com/m=hiscore_oldschool/overall.ws?table={0}&page='.format(skill)\n",
    "    #player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='\n",
    "    \n",
    "    # range of pages to scrape results\n",
    "    min_page = min_rank//25\n",
    "    max_page = max_rank//25\n",
    "    pages = max_page - min_page\n",
    "    #url_no_page = ranking_url_base + 'table={22}&page='.format(table_num)\n",
    "    \n",
    "    for i in range(min_page, max_page):\n",
    "        \n",
    "        url = total_base + str(i)\n",
    "        soup = BeautifulSoup(requests.get(url).text)\n",
    "        try:\n",
    "            table = soup.find(\"tbody\")\n",
    "        # create catch here\n",
    "        except:\n",
    "            return soup\n",
    "        \n",
    "        for row in table.findAll(\"tr\"):\n",
    "            player = row.findAll(\"td\")\n",
    "            # cols = [rank, name, level, xp]\n",
    "            cols = [element.text.strip() for element in player]\n",
    "            #name = cols[1].replace(u'\\xa0', u' ')\n",
    "            \n",
    "            if cols[1]:\n",
    "                name = cols[1].replace(u'\\xa0', u' ')\n",
    "                #print(name)\n",
    "                rank = int(cols[0].replace(u',', u''))\n",
    "                try:\n",
    "                    conn.execute('''INSERT OR IGNORE INTO players(name, date, skill, rank) VALUES (?,?,?,?)''',(name, time, skill_name, int(rank)))\n",
    "                    conn.commit()\n",
    "                except:\n",
    "                    print('Failed:', cols)\n",
    "                    continue\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        if not i%20:\n",
    "            print('Pages scraped: {0}/{1}'.format(i-min_page, pages), datetime.utcnow() - now)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages scraped: 0/3040 2019-05-23 00:42:32.248702\n",
      "Pages scraped: 10/3040 2019-05-23 00:42:52.417858\n",
      "Pages scraped: 90/3040 2019-05-23 00:47:09.248114\n",
      "Pages scraped: 110/3040 2019-05-23 00:48:39.260316\n",
      "Pages scraped: 130/3040 2019-05-23 00:50:06.607790\n",
      "Pages scraped: 150/3040 2019-05-23 00:51:33.753334\n",
      "Pages scraped: 160/3040 2019-05-23 00:52:19.528423\n",
      "Pages scraped: 180/3040 2019-05-23 00:53:51.589447\n",
      "Pages scraped: 250/3040 2019-05-23 00:58:54.393519\n",
      "Pages scraped: 280/3040 2019-05-23 01:01:07.680285\n",
      "Pages scraped: 300/3040 2019-05-23 01:02:33.434284\n",
      "Pages scraped: 310/3040 2019-05-23 01:03:18.902874\n",
      "Pages scraped: 430/3040 2019-05-23 01:12:08.331737\n",
      "Pages scraped: 440/3040 2019-05-23 01:12:48.781933\n",
      "Pages scraped: 450/3040 2019-05-23 01:13:35.068626\n",
      "Pages scraped: 470/3040 2019-05-23 01:15:06.100727\n",
      "Pages scraped: 480/3040 2019-05-23 01:15:51.974203\n",
      "Pages scraped: 500/3040 2019-05-23 01:17:18.609096\n",
      "Pages scraped: 600/3040 2019-05-23 01:24:39.643897\n",
      "Pages scraped: 620/3040 2019-05-23 01:26:18.975955\n",
      "Pages scraped: 680/3040 2019-05-23 01:30:38.570244\n",
      "Pages scraped: 690/3040 2019-05-23 01:31:19.121981\n",
      "Pages scraped: 700/3040 2019-05-23 01:32:04.282481\n",
      "Pages scraped: 710/3040 2019-05-23 01:32:49.854168\n",
      "Pages scraped: 730/3040 2019-05-23 01:34:18.425911\n",
      "Pages scraped: 760/3040 2019-05-23 01:36:35.648994\n",
      "Pages scraped: 770/3040 2019-05-23 01:37:21.219852\n",
      "Pages scraped: 850/3040 2019-05-23 01:43:09.267112\n",
      "Pages scraped: 870/3040 2019-05-23 01:44:36.717205\n",
      "Pages scraped: 880/3040 2019-05-23 01:45:22.592324\n",
      "Pages scraped: 890/3040 2019-05-23 01:46:03.450991\n",
      "Pages scraped: 900/3040 2019-05-23 01:46:48.915323\n",
      "Pages scraped: 980/3040 2019-05-23 01:52:39.646262\n",
      "Pages scraped: 1000/3040 2019-05-23 01:54:05.052348\n",
      "Pages scraped: 1010/3040 2019-05-23 01:54:49.903425\n",
      "Pages scraped: 1090/3040 2019-05-23 02:00:39.101755\n",
      "Pages scraped: 1110/3040 2019-05-23 02:02:05.323085\n",
      "Pages scraped: 1120/3040 2019-05-23 02:02:50.788948\n",
      "Pages scraped: 1200/3040 2019-05-23 02:08:39.779021\n",
      "Pages scraped: 1220/3040 2019-05-23 02:10:06.312838\n",
      "Pages scraped: 1240/3040 2019-05-23 02:11:33.762198\n",
      "Pages scraped: 1250/3040 2019-05-23 02:12:19.746465\n",
      "Pages scraped: 1350/3040 2019-05-23 02:19:39.358279\n",
      "Pages scraped: 1370/3040 2019-05-23 02:21:07.339486\n",
      "Pages scraped: 1380/3040 2019-05-23 02:21:48.491625\n",
      "Pages scraped: 1390/3040 2019-05-23 02:22:34.982584\n",
      "Pages scraped: 1400/3040 2019-05-23 02:23:20.961867\n",
      "Pages scraped: 1430/3040 2019-05-23 02:25:39.207864\n",
      "Pages scraped: 1450/3040 2019-05-23 02:27:07.786251\n",
      "Pages scraped: 1480/3040 2019-05-23 02:29:18.860441\n",
      "Pages scraped: 1590/3040 2019-05-23 02:37:24.560275\n",
      "Pages scraped: 1620/3040 2019-05-23 02:39:36.865754\n",
      "Pages scraped: 1640/3040 2019-05-23 02:41:04.523556\n",
      "Pages scraped: 1650/3040 2019-05-23 02:41:50.502901\n",
      "Pages scraped: 1670/3040 2019-05-23 02:43:24.407169\n",
      "Pages scraped: 1700/3040 2019-05-23 02:45:36.507640\n",
      "Pages scraped: 1710/3040 2019-05-23 02:46:22.277186\n",
      "Pages scraped: 1810/3040 2019-05-23 02:53:37.803639\n",
      "Pages scraped: 1820/3040 2019-05-23 02:54:18.555737\n",
      "Pages scraped: 1830/3040 2019-05-23 02:55:03.923470\n",
      "Pages scraped: 1840/3040 2019-05-23 02:55:49.288732\n",
      "Pages scraped: 1920/3040 2019-05-23 03:01:38.688541\n",
      "Pages scraped: 1930/3040 2019-05-23 03:02:19.447409\n",
      "Pages scraped: 1940/3040 2019-05-23 03:03:05.633496\n",
      "Pages scraped: 1950/3040 2019-05-23 03:03:51.300234\n",
      "Pages scraped: 2040/3040 2019-05-23 03:10:24.633176\n",
      "Pages scraped: 2070/3040 2019-05-23 03:12:36.119876\n",
      "Pages scraped: 2080/3040 2019-05-23 03:13:21.586848\n",
      "Pages scraped: 2180/3040 2019-05-23 03:20:37.515503\n",
      "Pages scraped: 2200/3040 2019-05-23 03:22:03.842340\n",
      "Pages scraped: 2220/3040 2019-05-23 03:23:36.720752\n",
      "Pages scraped: 2230/3040 2019-05-23 03:24:22.085390\n",
      "Pages scraped: 2310/3040 2019-05-23 03:30:08.824667\n",
      "Pages scraped: 2330/3040 2019-05-23 03:31:36.387068\n",
      "Pages scraped: 2340/3040 2019-05-23 03:32:22.460849\n",
      "Pages scraped: 2350/3040 2019-05-23 03:33:03.526869\n",
      "Pages scraped: 2360/3040 2019-05-23 03:33:49.499303\n",
      "Pages scraped: 2460/3040 2019-05-23 03:41:06.457701\n",
      "Pages scraped: 2470/3040 2019-05-23 03:41:51.719846\n",
      "Pages scraped: 2560/3040 2019-05-23 03:48:24.538605\n",
      "Pages scraped: 2570/3040 2019-05-23 03:49:09.596735\n",
      "Pages scraped: 2590/3040 2019-05-23 03:50:38.050681\n",
      "Pages scraped: 2610/3040 2019-05-23 03:52:03.672098\n",
      "Pages scraped: 2620/3040 2019-05-23 03:52:49.226724\n",
      "Pages scraped: 2700/3040 2019-05-23 03:58:37.186350\n",
      "Pages scraped: 2710/3040 2019-05-23 03:59:22.138493\n",
      "Pages scraped: 2880/3040 2019-05-23 04:11:54.639228\n",
      "Pages scraped: 2890/3040 2019-05-23 04:12:39.185666\n",
      "Pages scraped: 2910/3040 2019-05-23 04:14:04.694256\n",
      "Pages scraped: 2920/3040 2019-05-23 04:14:49.956095\n",
      "Pages scraped: 3000/3040 2019-05-23 04:20:38.843720\n",
      "Pages scraped: 3010/3040 2019-05-23 04:21:19.500008\n",
      "Pages scraped: 3020/3040 2019-05-23 04:22:05.271707\n",
      "Pages scraped: 3030/3040 2019-05-23 04:22:50.944870\n"
     ]
    }
   ],
   "source": [
    "# scrape hiscores for names (construction ranks 440000 to 561000, level 50 con)\n",
    "try:\n",
    "    import importlib\n",
    "    importlib.reload(scrape_hs)\n",
    "    print('Reimported scrape_hs.py')\n",
    "except:\n",
    "    import scrape_hs\n",
    "\n",
    "# 1400 end, 35000 done\n",
    "min_rank = 485000 #440000 min originally\n",
    "max_rank = 561000\n",
    "skill = 23 # construction = 23\n",
    "\n",
    "table = scrape_hs.scrape_names(min_rank, max_rank, skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to top</a>  \n",
    "### <a name='scrape_skills'> Scrape skills of each player </a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas\n",
    "import pyspark\n",
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import pickle as pckl\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "conn = sql.connect('data/osrs.db')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"osrs\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sqlContext = pyspark.sql.SQLContext(spark)#sc = pyspark.SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(players):\n",
    "    return (players, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=Row(_1='Supreme_v1')),\n",
       " Row(name=Row(_1='M3EK MlLLY')),\n",
       " Row(name=Row(_1='MrTrunks')),\n",
       " Row(name=Row(_1='OBG Nicola')),\n",
       " Row(name=Row(_1='redromper')),\n",
       " Row(name=Row(_1='SSGvegito')),\n",
       " Row(name=Row(_1='YES vote')),\n",
       " Row(name=Row(_1='TMGGuthan')),\n",
       " Row(name=Row(_1='Kyle SR')),\n",
       " Row(name=Row(_1='Von Disney')),\n",
       " Row(name=Row(_1='thafamilia3')),\n",
       " Row(name=Row(_1='buga shuga')),\n",
       " Row(name=Row(_1='sweetslaps')),\n",
       " Row(name=Row(_1='lceef')),\n",
       " Row(name=Row(_1='La Tigers 44')),\n",
       " Row(name=Row(_1='Zephozzz')),\n",
       " Row(name=Row(_1='Grant2k1')),\n",
       " Row(name=Row(_1='CleanedTotal')),\n",
       " Row(name=Row(_1='Space Mankey')),\n",
       " Row(name=Row(_1='Rongdre')),\n",
       " Row(name=Row(_1='Stab Em All2')),\n",
       " Row(name=Row(_1='Hellz Zerker')),\n",
       " Row(name=Row(_1='ChrisW247')),\n",
       " Row(name=Row(_1='Logless')),\n",
       " Row(name=Row(_1='M d m a zing')),\n",
       " Row(name=Row(_1='PhantomsCore')),\n",
       " Row(name=Row(_1='young pimpin')),\n",
       " Row(name=Row(_1='Maximus Zerk')),\n",
       " Row(name=Row(_1='L55')),\n",
       " Row(name=Row(_1='Badger64'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sql.connect('data/osrs.db')\n",
    "names = conn.execute('''SELECT name FROM players_name LIMIT 30''').fetchall()\n",
    "df = pd.DataFrame({'name': names})\n",
    "sdf = sqlContext.createDataFrame(df)\n",
    "sdf2 = sdf.select('name').repartition(4)\n",
    "\n",
    "a = sdf2.rdd.mapPartitions(test).collect()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_player_part(players):\n",
    "    from datetime import datetime\n",
    "    # base url for api request\n",
    "    player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='\n",
    "\n",
    "    i = 0\n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    for player in players:\n",
    "        name = player['name']\n",
    "        url = player_url_base + name\n",
    "        \n",
    "        if not i%100:\n",
    "            now = datetime.utcnow()\n",
    "        else:\n",
    "            pass\n",
    "        i += 1\n",
    "        \n",
    "        try:\n",
    "            page = requests.get(url).text.replace(u'\\n', u' ')\n",
    "            skills = [i.split(',') for i in page.split()]\n",
    "            xp = [i[2] for i in skills[0:24]]\n",
    "            rank = [i[0] for i in skills[0:24]]\n",
    "            yield tuple((name, now, xp, rank))# players_data\n",
    "        except:\n",
    "            continue\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hellz Zerker', '2019-05-22 19:34:35')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = conn.execute('SELECT name, date FROM players_name LIMIT 50').fetchall()\n",
    "players[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = conn.execute('SELECT name, date FROM players_name').fetchall()\n",
    "players_df = pd.DataFrame({'name': [x[0] for x in players], 'date': [x[1] for x in players]})\n",
    "players_sdf = sqlContext.createDataFrame(players_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint enabled:  True \n",
      "Num partitions:  16\n"
     ]
    }
   ],
   "source": [
    "players = players_sdf.select('name').repartition(16)\n",
    "players_hs = players.rdd.mapPartitions(get_single_player_part)\n",
    "players_hs.is_checkpointed = True\n",
    "print('Checkpoint enabled: ', players_hs.is_checkpointed,'\\nNum partitions: ', players_hs.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-30 03:17:51.603598\n",
      "2019-05-31 07:54:07.501399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Ms Jinx',\n",
       " datetime.datetime(2019, 5, 30, 3, 17, 54, 48099),\n",
       " ['11968400',\n",
       "  '1122847',\n",
       "  '757863',\n",
       "  '738270',\n",
       "  '1167841',\n",
       "  '785925',\n",
       "  '366463',\n",
       "  '729736',\n",
       "  '1121586',\n",
       "  '641689',\n",
       "  '297706',\n",
       "  '572190',\n",
       "  '192952',\n",
       "  '741736',\n",
       "  '345522',\n",
       "  '274134',\n",
       "  '244771',\n",
       "  '184051',\n",
       "  '308406',\n",
       "  '574393',\n",
       "  '399890',\n",
       "  '106458',\n",
       "  '184845',\n",
       "  '109126'],\n",
       " ['443845',\n",
       "  '614330',\n",
       "  '638305',\n",
       "  '972125',\n",
       "  '797633',\n",
       "  '907926',\n",
       "  '503780',\n",
       "  '826204',\n",
       "  '371592',\n",
       "  '558650',\n",
       "  '666263',\n",
       "  '527408',\n",
       "  '709161',\n",
       "  '361890',\n",
       "  '432763',\n",
       "  '691228',\n",
       "  '384239',\n",
       "  '717141',\n",
       "  '303674',\n",
       "  '522748',\n",
       "  '337210',\n",
       "  '330905',\n",
       "  '485324',\n",
       "  '459787'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datetime.utcnow())\n",
    "players_results2 = players_hs.collect()\n",
    "print(datetime.utcnow())\n",
    "players_results2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pckl.dump(players_results3, open('data/players_results0601.pckl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-31 07:54:08.876105\n",
      "2019-06-01 12:44:02.238793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Ms Jinx',\n",
       " datetime.datetime(2019, 5, 31, 7, 54, 11, 100306),\n",
       " ['11968400',\n",
       "  '1122847',\n",
       "  '757863',\n",
       "  '738270',\n",
       "  '1167841',\n",
       "  '785925',\n",
       "  '366463',\n",
       "  '729736',\n",
       "  '1121586',\n",
       "  '641689',\n",
       "  '297706',\n",
       "  '572190',\n",
       "  '192952',\n",
       "  '741736',\n",
       "  '345522',\n",
       "  '274134',\n",
       "  '244771',\n",
       "  '184051',\n",
       "  '308406',\n",
       "  '574393',\n",
       "  '399890',\n",
       "  '106458',\n",
       "  '184845',\n",
       "  '109126'],\n",
       " ['444319',\n",
       "  '614864',\n",
       "  '638913',\n",
       "  '972847',\n",
       "  '798341',\n",
       "  '908676',\n",
       "  '504331',\n",
       "  '827160',\n",
       "  '372013',\n",
       "  '559032',\n",
       "  '666833',\n",
       "  '527872',\n",
       "  '709905',\n",
       "  '362367',\n",
       "  '433385',\n",
       "  '691906',\n",
       "  '384682',\n",
       "  '717868',\n",
       "  '304011',\n",
       "  '523313',\n",
       "  '337749',\n",
       "  '331297',\n",
       "  '485842',\n",
       "  '460365'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datetime.utcnow())\n",
    "players_results3 = players_hs.collect()\n",
    "print(datetime.utcnow())\n",
    "players_results3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reimported scrape_hs\n",
      "Checkpoint enabled:  True \n",
      "Num partitions:  16\n",
      "2019-05-28 21:04:03.673500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-3e72d9ba98f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mscrape_hs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mxp_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_hs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_players\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "try:\n",
    "    importlib.reload(scrape_hs)\n",
    "    print('Reimported scrape_hs')\n",
    "except:\n",
    "    import scrape_hs\n",
    "\n",
    "xp_df, rank_df = scrape_hs.get_all_players()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pckl\n",
    "#results = pckl.load(open('data/players_results1.pckl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z A T 0 XX',\n",
       " datetime.datetime(2019, 5, 24, 0, 54, 9, 51961),\n",
       " 14392616,\n",
       " 309851,\n",
       " 65507,\n",
       " 2816878,\n",
       " 1667719,\n",
       " 1977078,\n",
       " 125434,\n",
       " 1484428,\n",
       " 783341,\n",
       " 201439,\n",
       " 15791,\n",
       " 166138,\n",
       " 116305,\n",
       " 318242,\n",
       " 182060,\n",
       " 286286,\n",
       " 62756,\n",
       " 215569,\n",
       " 305446,\n",
       " 168996,\n",
       " 46663,\n",
       " 21961,\n",
       " 2949725,\n",
       " 105003]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = results[0:2].copy()\n",
    "for row in a:\n",
    "    xp = [row[0]] + [row[1]] + [int(x) for x in row[2]]\n",
    "    rank = [row[0]] + [row[1]] + [int(x) for x in row[3]]\n",
    "    \n",
    "xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.load('data/player_names.parquet').repartition(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pckl.dump(players_results, open('data/players_results528.pckl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>overall</th>\n",
       "      <th>attack</th>\n",
       "      <th>defence</th>\n",
       "      <th>strength</th>\n",
       "      <th>hitpoints</th>\n",
       "      <th>ranged</th>\n",
       "      <th>prayer</th>\n",
       "      <th>magic</th>\n",
       "      <th>...</th>\n",
       "      <th>smithing</th>\n",
       "      <th>mining</th>\n",
       "      <th>herblore</th>\n",
       "      <th>agility</th>\n",
       "      <th>thieving</th>\n",
       "      <th>slayer</th>\n",
       "      <th>farming</th>\n",
       "      <th>runecraft</th>\n",
       "      <th>hunter</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mjpel3</td>\n",
       "      <td>2019-05-24 00:54:09.051961</td>\n",
       "      <td>16294050</td>\n",
       "      <td>1031276</td>\n",
       "      <td>860741</td>\n",
       "      <td>1528457</td>\n",
       "      <td>2219614</td>\n",
       "      <td>3259345</td>\n",
       "      <td>744358</td>\n",
       "      <td>1458143</td>\n",
       "      <td>...</td>\n",
       "      <td>104648</td>\n",
       "      <td>749672</td>\n",
       "      <td>79803</td>\n",
       "      <td>524779</td>\n",
       "      <td>169396</td>\n",
       "      <td>498128</td>\n",
       "      <td>274860</td>\n",
       "      <td>120365</td>\n",
       "      <td>125401</td>\n",
       "      <td>102478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z A T 0 XX</td>\n",
       "      <td>2019-05-24 00:54:09.051961</td>\n",
       "      <td>14392616</td>\n",
       "      <td>309851</td>\n",
       "      <td>65507</td>\n",
       "      <td>2816878</td>\n",
       "      <td>1667719</td>\n",
       "      <td>1977078</td>\n",
       "      <td>125434</td>\n",
       "      <td>1484428</td>\n",
       "      <td>...</td>\n",
       "      <td>182060</td>\n",
       "      <td>286286</td>\n",
       "      <td>62756</td>\n",
       "      <td>215569</td>\n",
       "      <td>305446</td>\n",
       "      <td>168996</td>\n",
       "      <td>46663</td>\n",
       "      <td>21961</td>\n",
       "      <td>2949725</td>\n",
       "      <td>105003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elliebus23</td>\n",
       "      <td>2019-05-24 00:54:09.051961</td>\n",
       "      <td>48627220</td>\n",
       "      <td>2476953</td>\n",
       "      <td>2054502</td>\n",
       "      <td>2026809</td>\n",
       "      <td>3650780</td>\n",
       "      <td>2473785</td>\n",
       "      <td>892909</td>\n",
       "      <td>2586039</td>\n",
       "      <td>...</td>\n",
       "      <td>131354</td>\n",
       "      <td>480326</td>\n",
       "      <td>88082</td>\n",
       "      <td>796701</td>\n",
       "      <td>154521</td>\n",
       "      <td>1902393</td>\n",
       "      <td>262975</td>\n",
       "      <td>81436</td>\n",
       "      <td>1999849</td>\n",
       "      <td>106951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SloppySlay</td>\n",
       "      <td>2019-05-24 00:54:09.051961</td>\n",
       "      <td>13665012</td>\n",
       "      <td>2084263</td>\n",
       "      <td>1557064</td>\n",
       "      <td>1441797</td>\n",
       "      <td>2289488</td>\n",
       "      <td>1759085</td>\n",
       "      <td>1989926</td>\n",
       "      <td>1261546</td>\n",
       "      <td>...</td>\n",
       "      <td>12743</td>\n",
       "      <td>22329</td>\n",
       "      <td>4045</td>\n",
       "      <td>29972</td>\n",
       "      <td>97299</td>\n",
       "      <td>485469</td>\n",
       "      <td>0</td>\n",
       "      <td>16932</td>\n",
       "      <td>3129</td>\n",
       "      <td>102327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dylan Is Bad</td>\n",
       "      <td>2019-05-24 00:54:09.051961</td>\n",
       "      <td>33228886</td>\n",
       "      <td>3351083</td>\n",
       "      <td>2101649</td>\n",
       "      <td>5381825</td>\n",
       "      <td>5410943</td>\n",
       "      <td>4224400</td>\n",
       "      <td>759214</td>\n",
       "      <td>5746639</td>\n",
       "      <td>...</td>\n",
       "      <td>46939</td>\n",
       "      <td>302305</td>\n",
       "      <td>29436</td>\n",
       "      <td>826671</td>\n",
       "      <td>168041</td>\n",
       "      <td>2125762</td>\n",
       "      <td>37583</td>\n",
       "      <td>51158</td>\n",
       "      <td>273837</td>\n",
       "      <td>102872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name                       date   overall   attack  defence  \\\n",
       "0        Mjpel3 2019-05-24 00:54:09.051961  16294050  1031276   860741   \n",
       "1    Z A T 0 XX 2019-05-24 00:54:09.051961  14392616   309851    65507   \n",
       "2    Elliebus23 2019-05-24 00:54:09.051961  48627220  2476953  2054502   \n",
       "3    SloppySlay 2019-05-24 00:54:09.051961  13665012  2084263  1557064   \n",
       "4  Dylan Is Bad 2019-05-24 00:54:09.051961  33228886  3351083  2101649   \n",
       "\n",
       "  strength hitpoints   ranged   prayer    magic  ... smithing  mining  \\\n",
       "0  1528457   2219614  3259345   744358  1458143  ...   104648  749672   \n",
       "1  2816878   1667719  1977078   125434  1484428  ...   182060  286286   \n",
       "2  2026809   3650780  2473785   892909  2586039  ...   131354  480326   \n",
       "3  1441797   2289488  1759085  1989926  1261546  ...    12743   22329   \n",
       "4  5381825   5410943  4224400   759214  5746639  ...    46939  302305   \n",
       "\n",
       "  herblore agility thieving   slayer farming runecraft   hunter construction  \n",
       "0    79803  524779   169396   498128  274860    120365   125401       102478  \n",
       "1    62756  215569   305446   168996   46663     21961  2949725       105003  \n",
       "2    88082  796701   154521  1902393  262975     81436  1999849       106951  \n",
       "3     4045   29972    97299   485469       0     16932     3129       102327  \n",
       "4    29436  826671   168041  2125762   37583     51158   273837       102872  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_xp = pckl.load(open('data/players_xp.pckl', 'rb'))\n",
    "players_xp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_players = players_results[0:100].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"name\", \"date\", \"Overall\", \"Attack\", \"Defence\", \"Strength\", \"Hitpoints\", \"Ranged\", \"Prayer\", \"Magic\", \"Cooking\",\n",
    "              \"Woodcutting\", \"Fletching\", \"Fishing\", \"Firemaking\", \"Crafting\", \"Smithing\", \"Mining\", \"Herblore\",\n",
    "              \"Agility\", \"Thieving\", \"Slayer\", \"Farming\", \"Runecraft\", \"Hunter\", \"Construction\"]\n",
    "columns = [x.lower() for x in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140379"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = players_results + players_results2\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_df(data, df, table):\n",
    "    \n",
    "    columns = ['name', 'date', 'overall', 'attack', 'defence', 'strength', 'hitpoints', 'ranged', 'prayer', 'magic', \n",
    "               'cooking', 'woodcutting', 'fletching', 'fishing', 'firemaking', 'crafting', 'smithing', 'mining', \n",
    "               'herblore', 'agility', 'thieving', 'slayer', 'farming', 'runecraft', 'hunter', 'construction']\n",
    "    \n",
    "    # which data to parse, rank = 3rd entry in tuple, xp = 2nd entry in tuple\n",
    "    if table == 'rank':\n",
    "        keep = 3\n",
    "    else:\n",
    "        keep = 2\n",
    "        \n",
    "    data_list = []\n",
    "        \n",
    "    for row in data:\n",
    "        insert = [row[0]] + [row[1]] + [int(x) for x in row[keep]]\n",
    "        key_value = zip(columns, insert)\n",
    "        \n",
    "        my_dict = {}\n",
    "    \n",
    "        for key, value in key_value:\n",
    "            my_dict[key] = value\n",
    "            \n",
    "        data_list.append(my_dict)\n",
    "    \n",
    "    df = df.append(data_list)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_days = data_to_df(results, players_xp, 'xp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pckl.dump(two_days, open('data/players_xp.pckl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp_df = sqlContext.createDataFrame(players_xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Mjpel3', date=datetime.datetime(2019, 5, 24, 0, 54, 9, 51961), overall=16294050, attack=1031276, defence=860741, strength=1528457, hitpoints=2219614, ranged=3259345, prayer=744358, magic=1458143, cooking=769577, woodcutting=588335, fletching=127612, fishing=676967, firemaking=106485, crafting=173610, smithing=104648, mining=749672, herblore=79803, agility=524779, thieving=169396, slayer=498128, farming=274860, runecraft=120365, hunter=125401, construction=102478),\n",
       " Row(name='Z A T 0 XX', date=datetime.datetime(2019, 5, 24, 0, 54, 9, 51961), overall=14392616, attack=309851, defence=65507, strength=2816878, hitpoints=1667719, ranged=1977078, prayer=125434, magic=1484428, cooking=783341, woodcutting=201439, fletching=15791, fishing=166138, firemaking=116305, crafting=318242, smithing=182060, mining=286286, herblore=62756, agility=215569, thieving=305446, slayer=168996, farming=46663, runecraft=21961, hunter=2949725, construction=105003),\n",
       " Row(name='Elliebus23', date=datetime.datetime(2019, 5, 24, 0, 54, 9, 51961), overall=48627220, attack=2476953, defence=2054502, strength=2026809, hitpoints=3650780, ranged=2473785, prayer=892909, magic=2586039, cooking=14835750, woodcutting=482458, fletching=2193075, fishing=8549601, firemaking=111317, crafting=288654, smithing=131354, mining=480326, herblore=88082, agility=796701, thieving=154521, slayer=1902393, farming=262975, runecraft=81436, hunter=1999849, construction=106951),\n",
       " Row(name='SloppySlay', date=datetime.datetime(2019, 5, 24, 0, 54, 9, 51961), overall=13665012, attack=2084263, defence=1557064, strength=1441797, hitpoints=2289488, ranged=1759085, prayer=1989926, magic=1261546, cooking=18605, woodcutting=31855, fletching=10310, fishing=173530, firemaking=102020, crafting=171278, smithing=12743, mining=22329, herblore=4045, agility=29972, thieving=97299, slayer=485469, farming=0, runecraft=16932, hunter=3129, construction=102327),\n",
       " Row(name='Dylan Is Bad', date=datetime.datetime(2019, 5, 24, 0, 54, 9, 51961), overall=33228886, attack=3351083, defence=2101649, strength=5381825, hitpoints=5410943, ranged=4224400, prayer=759214, magic=5746639, cooking=764455, woodcutting=311758, fletching=47219, fishing=141104, firemaking=284278, crafting=739715, smithing=46939, mining=302305, herblore=29436, agility=826671, thieving=168041, slayer=2125762, farming=37583, runecraft=51158, hunter=273837, construction=102872)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=#skillstable></a>  \n",
    "<a href=#top>Back to top</a>  \n",
    "  \n",
    "__Tables (Skills)__  \n",
    "  \n",
    "0 - Overall  \n",
    "1 - Attack  \n",
    "2 - Defence  \n",
    "3 - Strength  \n",
    "4 - Hitpoints  \n",
    "5 - Ranged  \n",
    "6 - Prayer  \n",
    "7 - Magic  \n",
    "8 - Cooking  \n",
    "9 - Woodcutting  \n",
    "10 - Fletching  \n",
    "11 - Fishing  \n",
    "12 - Firemaking  \n",
    "13 - Crafting  \n",
    "14 - Smithing  \n",
    "15 - Mining  \n",
    "16 - Herblore  \n",
    "17 - Agility  \n",
    "18 - Thieving  \n",
    "19 - Slayer  \n",
    "20 - Farming  \n",
    "21 - Runecraft  \n",
    "22 - Hunter  \n",
    "23 - Construction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>Back to top</a>  \n",
    "### <a name='old'> Deprecated worked, since refactored </a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tyler', 100, 'att')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Tyler'\n",
    "xp = 100\n",
    "skill = 'att'\n",
    "tuple((name, xp, skill))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='IDK Hunter'),\n",
       " Row(name='Evil rake'),\n",
       " Row(name='Hoibur'),\n",
       " Row(name='madnesh'),\n",
       " Row(name='AGM-86')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = df.select('name').limit(5)\n",
    "players.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for i in range(len(players_results)):\n",
    "    total += len(players_results[i])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_player(players):\n",
    "    from datetime import datetime\n",
    "    #print(player['name'])\n",
    "    # base url for api request\n",
    "    player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='\n",
    "    \n",
    "    for player in players:\n",
    "        name = player['name']\n",
    "        url = player_url_base + name\n",
    "    \n",
    "        now = datetime.utcnow()\n",
    "        time = '{0}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        \n",
    "        try:\n",
    "            page = requests.get(url).text.replace(u'\\n', u' ')\n",
    "            skills = [i.split(',') for i in page.split()]\n",
    "            xp = [i[2] for i in skills[0:24]]\n",
    "            rank = [i[0] for i in skills[0:24]]\n",
    "        except:\n",
    "            print(name)\n",
    "            continue\n",
    "    return (name, time, xp, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player(conn, skill):\n",
    "    # base url for api request\n",
    "    player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='\n",
    "    \n",
    "    skills = [\"Overall\", \"Attack\", \"Defence\", \"Strength\", \"Hitpoints\", \"Ranged\", \"Prayer\", \"Magic\", \"Cooking\",\n",
    "              \"Woodcutting\", \"Fletching\", \"Fishing\", \"Firemaking\", \"Crafting\", \"Smithing\", \"Mining\", \"Herblore\",\n",
    "              \"Agility\", \"Thieving\", \"Slayer\", \"Farming\", \"Runecraft\", \"Hunter\", \"Construction\"]\n",
    "    skill_name = skills[skill]\n",
    "    print(skill_name)\n",
    "    \n",
    "    players = conn.execute('''SELECT name from players_name WHERE skill = (?)''', (skill_name,)).fetchall()\n",
    "    num_players = len(players)\n",
    "    \n",
    "    now = datetime.utcnow()\n",
    "    time = '{0}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(now.year, now.month, now.day, \n",
    "                                                                now.hour, now.minute, now.second)\n",
    "    failed = []\n",
    "    num = 0\n",
    "    \n",
    "    for player in players:\n",
    "        # sql returns list of tuples, this isolates the name\n",
    "        name = player[0]\n",
    "        url = player_url_base + name\n",
    "        \n",
    "        num += 1\n",
    "        if not num%20:\n",
    "            print('Players scraped: {0}/{1}'.format(i, num_players), datetime.utcnow() - now)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            page = requests.get(url).text.replace(u'\\n', u' ')\n",
    "            skills = [i.split(',') for i in page.split()]\n",
    "            xp = [i[2] for i in skills[0:24]]\n",
    "            rank = [i[0] for i in skills[0:24]]\n",
    "        except:\n",
    "            print(name)\n",
    "            failed.append(name)\n",
    "            continue\n",
    "            \n",
    "        conn.execute('''INSERT INTO players_xp VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''', [name, time] + xp)\n",
    "        conn.execute('''INSERT INTO players_rank VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''', [name, time] + rank)\n",
    "            \n",
    "    return failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(date,DateType,true),StructField(skill,StringType,true),StructField(rank,LongType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get spark df schema\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast date type, no longer using this\n",
    "from pyspark.sql.types import DateType\n",
    "df = df.withColumn('date', df['date'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o166.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 11, localhost, executor driver): java.io.FileNotFoundException: File file:/Users/tylerblair/Documents/projects/bot_detection/data/player_names.parquet/part-00016-35538ec8-3aa5-4c02-9994-b09ed5ae9ea5-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/tylerblair/Documents/projects/bot_detection/data/player_names.parquet/part-00016-35538ec8-3aa5-4c02-9994-b09ed5ae9ea5-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4cbe78e18809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/player_names.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/osrs/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o166.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 11, localhost, executor driver): java.io.FileNotFoundException: File file:/Users/tylerblair/Documents/projects/bot_detection/data/player_names.parquet/part-00016-35538ec8-3aa5-4c02-9994-b09ed5ae9ea5-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/tylerblair/Documents/projects/bot_detection/data/player_names.parquet/part-00016-35538ec8-3aa5-4c02-9994-b09ed5ae9ea5-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.write.mode('overwrite').parquet('data/player_names.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get player names and insert into osrs.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_players_table(conn):\n",
    "    \n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS players(\n",
    "                    \"name\" TEXT PRIMARY KEY,\n",
    "                    \"date\" DATETIME NOT NULL,\n",
    "                    \"skill\" TEXT,\n",
    "                    \"rank\" INTEGER)''')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47365"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = conn.execute('''SELECT name from players_name''').fetchall()\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "conn.execute('''CREATE TABLE IF NOT EXISTS players_name(\n",
    "                    \"name\" TEXT, \n",
    "                    \"date\" DATETIME NOT NULL, \n",
    "                    \"skill\" TEXT, \n",
    "                    \"rank\" INTEGER)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'name', 'TEXT', 0, None, 0),\n",
       " (1, 'date', 'DATETIME', 1, None, 0),\n",
       " (2, 'skill', 'TEXT', 0, None, 0),\n",
       " (3, 'rank', 'INTEGER', 0, None, 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute('''PRAGMA table_info(\"players_name\")''').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>skill</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tyler</td>\n",
       "      <td>now</td>\n",
       "      <td>abc</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tyler</td>\n",
       "      <td>now</td>\n",
       "      <td>abc</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name date skill   rank\n",
       "0  Tyler  now   abc  100.0\n",
       "1  Tyler  now   abc  100.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.append({'name': 'Tyler', 'date': 'now', 'skill': 'abc', 'rank': 100}, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hellz Zerker', '2019-05-22 19:34:35', 'Construction', 439976),\n",
       " ('Grant2k1', '2019-05-22 19:34:35', 'Construction', 439977),\n",
       " ('Kyle SR', '2019-05-22 19:34:35', 'Construction', 439978),\n",
       " ('Supreme_v1', '2019-05-22 19:34:35', 'Construction', 439979),\n",
       " ('M3EK MlLLY', '2019-05-22 19:34:35', 'Construction', 439980),\n",
       " ('Von Disney', '2019-05-22 19:34:35', 'Construction', 439981),\n",
       " ('ChrisW247', '2019-05-22 19:34:35', 'Construction', 439982),\n",
       " ('CleanedTotal', '2019-05-22 19:34:35', 'Construction', 439983),\n",
       " ('buga shuga', '2019-05-22 19:34:35', 'Construction', 439984),\n",
       " ('OBG Nicola', '2019-05-22 19:34:35', 'Construction', 439985),\n",
       " ('M d m a zing', '2019-05-22 19:34:35', 'Construction', 439986),\n",
       " ('MrTrunks', '2019-05-22 19:34:35', 'Construction', 439987),\n",
       " ('Logless', '2019-05-22 19:34:35', 'Construction', 439988),\n",
       " ('thafamilia3', '2019-05-22 19:34:35', 'Construction', 439989),\n",
       " ('SSGvegito', '2019-05-22 19:34:35', 'Construction', 439990),\n",
       " ('Space Mankey', '2019-05-22 19:34:35', 'Construction', 439991),\n",
       " ('PhantomsCore', '2019-05-22 19:34:35', 'Construction', 439992),\n",
       " ('young pimpin', '2019-05-22 19:34:35', 'Construction', 439993),\n",
       " ('lceef', '2019-05-22 19:34:35', 'Construction', 439994),\n",
       " ('sweetslaps', '2019-05-22 19:34:35', 'Construction', 439995),\n",
       " ('redromper', '2019-05-22 19:34:35', 'Construction', 439996),\n",
       " ('TMGGuthan', '2019-05-22 19:34:35', 'Construction', 439997),\n",
       " ('Stab Em All2', '2019-05-22 19:34:35', 'Construction', 439998),\n",
       " ('Badger64', '2019-05-22 19:34:35', 'Construction', 439999),\n",
       " ('YES vote', '2019-05-22 19:34:35', 'Construction', 440000)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute('''SELECT * from players''').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x114fcd730>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.close()\n",
    "conn = sql.connect('data/osrs.db')\n",
    "\n",
    "conn.execute('''DROP TABLE players''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1559069497"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MattsMauler',\n",
       "  'Zen Santa',\n",
       "  'Dr McGroober',\n",
       "  'Bird Brained',\n",
       "  'BoxOfCondums',\n",
       "  'Brother Boof',\n",
       "  'Typulsion',\n",
       "  'Boyo Bob',\n",
       "  'High Pro',\n",
       "  'Emperator',\n",
       "  'Mabaza',\n",
       "  'Tokkurainen',\n",
       "  'Akaaay',\n",
       "  'leathalchick',\n",
       "  'KinG CarrotI',\n",
       "  'Truitedulac3',\n",
       "  'Lightworks',\n",
       "  'Lukeed113',\n",
       "  '100 hp',\n",
       "  'Sushi Phil',\n",
       "  'HappinessLTD',\n",
       "  'RevToo',\n",
       "  'zdog520',\n",
       "  'Embezzlez',\n",
       "  'ICESKATE'],\n",
       " ['440,000', 'ICESKATE', '51', '112,073'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_hs_tables(conn):\n",
    "    \n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS player_xp (\n",
    "                    name TEXT PRIMARY KEY,\n",
    "                    date DATETIME NOT NULL,\n",
    "                    overall INTEGER,\n",
    "                    attack INTEGER,\n",
    "                    defence INTEGER,\n",
    "                    strength INTEGER,\n",
    "                    hitpoints INTEGER,\n",
    "                    ranged INTEGER,\n",
    "                    prayer INTEGER,\n",
    "                    magic INTEGER,\n",
    "                    cooking INTEGER,\n",
    "                    woodcutting INTEGER,\n",
    "                    fletching INTEGER,\n",
    "                    fishing INTEGER,\n",
    "                    firemaking INTEGER,\n",
    "                    crafting INTEGER,\n",
    "                    smithing INTEGER,\n",
    "                    mining INTEGER,\n",
    "                    herblore INTEGER,\n",
    "                    agility INTEGER,\n",
    "                    thieving INTEGER,\n",
    "                    slayer INTEGER,\n",
    "                    farming INTEGER,\n",
    "                    runecraft INTEGER,\n",
    "                    hunter INTEGER,\n",
    "                    construction INTEGER)''')\n",
    "\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS player_rank (\n",
    "                    name TEXT PRIMARY KEY,\n",
    "                    date DATETIME NOT NULL,\n",
    "                    overall INTEGER,\n",
    "                    attack INTEGER,\n",
    "                    defence INTEGER,\n",
    "                    strength INTEGER,\n",
    "                    hitpoints INTEGER,\n",
    "                    ranged INTEGER,\n",
    "                    prayer INTEGER,\n",
    "                    magic INTEGER,\n",
    "                    cooking INTEGER,\n",
    "                    woodcutting INTEGER,\n",
    "                    fletching INTEGER,\n",
    "                    fishing INTEGER,\n",
    "                    firemaking INTEGER,\n",
    "                    crafting INTEGER,\n",
    "                    smithing INTEGER,\n",
    "                    mining INTEGER,\n",
    "                    herblore INTEGER,\n",
    "                    agility INTEGER,\n",
    "                    thieving INTEGER,\n",
    "                    slayer INTEGER,\n",
    "                    farming INTEGER,\n",
    "                    runecraft INTEGER,\n",
    "                    hunter INTEGER,\n",
    "                    construction INTEGER)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-05-22 18:05:33'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not truncating time anymore\n",
    "time = '{0}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_num from skills list above, e.g., table 9 = woodcutting\n",
    "def scrape_rank(table_num, num_players):\n",
    "    \n",
    "    conn = sql.connect('osrs_hs.db')\n",
    "    \n",
    "    max_pages = num_players//25\n",
    "    url_no_page = ranking_url_base + 'table={0}&page='.format(table_num)\n",
    "    \n",
    "    for i in range(max_pages):\n",
    "        url = url_no_page + str(i)\n",
    "        soup = BeautifulSoup(requests.get(url).text)\n",
    "        try:\n",
    "            table = soup.find(\"tbody\")\n",
    "        except:\n",
    "            return players\n",
    "        \n",
    "        for row in table.findAll(\"tr\"):\n",
    "            player = row.findAll(\"td\")\n",
    "            cols = [element.text.strip() for element in player]\n",
    "            players.append(cols[1])\n",
    "            \n",
    "    return players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base URLs for later queries\n",
    "# ranking URL ends in: table={0}&page={1} where table number is in order of skills list\n",
    "ranking_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/overall.ws?' \n",
    "player_url_base = 'https://secure.runescape.com/m=hiscore_oldschool/index_lite.ws?player='"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
